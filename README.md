<div align="center">
  <img src="banner.png" alt="ClaraCore Banner" width="100%">
</div>

# ClaraCore üöÄ

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Go Version](https://img.shields.io/badge/Go-1.23+-blue.svg)](https://golang.org/)
[![Platform](https://img.shields.io/badge/Platform-Windows%20%7C%20Linux%20%7C%20macOS-lightgrey.svg)](https://github.com/prave/ClaraCore)

> **Auto-setup for llama.cpp** - Point it at your GGUF models folder and get a complete AI inference server in seconds.

ClaraCore extends [llama-swap](https://github.com/mostlygeek/llama-swap) with intelligent automation, bringing zero-configuration setup to llama.cpp deployments.

## ‚ú® What's New in ClaraCore

While maintaining 100% compatibility with llama-swap, ClaraCore adds:

- üéØ **Auto-Setup Engine** - Automatic GGUF detection and configuration
- üîç **Smart Hardware Detection** - CUDA/ROCm/Vulkan/Metal optimization
- ‚¨áÔ∏è **Binary Management** - Automatic llama-server downloads
- ‚öôÔ∏è **Intelligent Configs** - Production-ready settings out of the box
- üöÄ **Speculative Decoding** - Automatic draft model pairing

## üöÄ Quick Start

```bash
# One command setup - that's it!
./claracore --models-folder /path/to/your/gguf/models --backend vulkan

# ClaraCore will:
# 1. Scan for GGUF files
# 2. Detect your hardware (GPUs, CUDA, etc.)
# 3. Download optimal binaries
# 4. Generate intelligent configuration
# 5. Start serving immediately
```

## üì¶ Installation

### Download Binaries

```bash
# Windows
curl -L -o claracore.exe https://github.com/prave/ClaraCore/releases/latest/download/claracore-windows-amd64.exe

# Linux
curl -L -o claracore https://github.com/prave/ClaraCore/releases/latest/download/claracore-linux-amd64
chmod +x claracore

# macOS
curl -L -o claracore https://github.com/prave/ClaraCore/releases/latest/download/claracore-darwin-amd64
chmod +x claracore
```

### Build from Source

```bash
git clone https://github.com/prave/ClaraCore.git
cd ClaraCore
go build -o claracore .
```

## üéõÔ∏è Core Features

All the power of llama-swap, plus intelligent automation:

### From llama-swap (unchanged)
- ‚úÖ OpenAI API compatible endpoints
- ‚úÖ Automatic model swapping on-demand
- ‚úÖ Multiple models with `groups`
- ‚úÖ Auto-unload with `ttl`
- ‚úÖ Web UI for monitoring
- ‚úÖ Docker/Podman support
- ‚úÖ Full llama.cpp server control

### ClaraCore Enhancements
- ‚úÖ Zero-configuration setup
- ‚úÖ Automatic binary downloads
- ‚úÖ Hardware capability detection
- ‚úÖ Intelligent resource allocation
- ‚úÖ Speculative decoding setup
- ‚úÖ Model metadata parsing

## üìñ Usage Examples

### Automatic Setup
```bash
# Just point to your models
./claracore --models-folder ~/models
```
### Manual Setup - for devices like strix halo and others who want to customize or setup without relying on auto-detection
```bash
# Create a config file
./claracore -ram 64 -vram 24 -backend cuda
# it will download the binaries and create a config file automatically and UI will have all the features needed to manage models
```

### API Usage
```bash
# List models
curl http://localhost:8080/v1/models

# Chat completion
curl http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "llama-3-8b",
    "messages": [{"role": "user", "content": "Hello!"}]
  }'
```

## üîß Configuration

ClaraCore generates configurations automatically, but you can customize everything:

```yaml
# Auto-generated by ClaraCore
models:
  "llama-3-70b":
    cmd: |
      binaries/llama-server/llama-server
      --model models/llama-3-70b-q4.gguf
      --host 127.0.0.1 --port ${PORT}
      --flash-attn auto -ngl 99
    draft: "llama-3-8b"  # Automatic speculative decoding
    proxy: "http://127.0.0.1:${PORT}"
    
groups:
  "large-models":
    swap: true
    exclusive: true
    members: ["llama-3-70b", "qwen-72b"]
```

## üôè Credits & Acknowledgments

**ClaraCore is built on [llama-swap](https://github.com/mostlygeek/llama-swap) by [@mostlygeek](https://github.com/mostlygeek)** 

This project extends llama-swap's excellent proxy architecture with automatic setup capabilities. Approximately 90% of the core functionality comes from the original llama-swap codebase. We're deeply grateful for @mostlygeek's work in creating such a solid foundation.

### Special Thanks To:
- **[@mostlygeek](https://github.com/mostlygeek)** - Creator of llama-swap
- **[llama.cpp team](https://github.com/ggerganov/llama.cpp)** - The inference engine
- **[Georgi Gerganov](https://github.com/ggerganov)** - Creator of llama.cpp

## ü§ù Contributing

We welcome contributions! Whether it's bug fixes, new features, or documentation improvements.

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## üìÑ License

MIT License - Same as llama-swap. See [LICENSE](LICENSE) for details.

## üîó Links

- [ClaraCore Issues](https://github.com/prave/ClaraCore/issues)
- [Original llama-swap](https://github.com/mostlygeek/llama-swap)
- [llama.cpp](https://github.com/ggerganov/llama.cpp)
- [Documentation Wiki](https://github.com/prave/ClaraCore/wiki)

---

<div align="center">
  
**Built with ‚ù§Ô∏è by the community, for the community**

*Standing on the shoulders of giants*

</div>
