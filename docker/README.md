# üöÄ ClaraCore Universal Container

## One Container, Every GPU

ClaraCore's universal container automatically detects and adapts to your hardware. No configuration needed!

```bash
# That's literally it
docker compose -f docker-compose.universal.yml up
```

## ‚ú® What Makes It Universal?

- **üîç Smart Detection**: Automatically finds NVIDIA, AMD, Intel, or falls back to CPU
- **üì¶ One Image**: Same container works everywhere - no special builds needed
- **‚ö° Optimized**: Always picks the fastest backend for your hardware
- **üõ†Ô∏è Override Ready**: Force specific backends when needed

## üéØ Supported Hardware

| Hardware | Auto-Detected | Backend Used | Performance |
|----------|---------------|--------------|-------------|
| NVIDIA GPU | ‚úÖ Yes | CUDA | Best |
| AMD GPU | ‚úÖ Yes | ROCm | Best |
| Intel/AMD/NVIDIA | ‚úÖ Yes | Vulkan | Great |
| No GPU / CPU only | ‚úÖ Yes | CPU | Good |

## üì¶ Quick Start

### 1. Build the Image

```bash
cd docker
./build-universal.sh  # Linux/Mac
# or
.\build-universal.ps1  # Windows
```

### 2. Add Your Models

```bash
# Copy your GGUF models to the models folder
cp /path/to/your/*.gguf ./models/
```

### 3. Start the Container

```bash
# Let it auto-detect your hardware (recommended)
docker compose -f docker-compose.universal.yml up

# The container will:
# 1. Detect your GPU (NVIDIA/AMD/Intel) or use CPU
# 2. Configure the optimal backend
# 3. Start ClaraCore with your models
```

### 4. Access the UI

Open your browser to: **http://localhost:5800/ui/**

## üéõÔ∏è Deployment Modes

### Auto-Detection (Recommended)

```bash
docker compose -f docker-compose.universal.yml up
```

The container automatically detects:
1. NVIDIA GPU ‚Üí Uses CUDA
2. AMD GPU ‚Üí Uses ROCm
3. Vulkan-capable GPU ‚Üí Uses Vulkan
4. No GPU ‚Üí Uses CPU

### Force Specific Backend

```bash
# Force CPU mode
docker compose -f docker-compose.cpu-only.yml up

# Force CUDA (NVIDIA)
docker compose -f docker-compose.cuda-explicit.yml up

# Force ROCm (AMD)
docker compose -f docker-compose.rocm-explicit.yml up

# Force Vulkan (Universal)
docker compose -f docker-compose.vulkan-explicit.yml up
```

### Custom Backend via Environment

```bash
docker run -e CLARACORE_BACKEND=vulkan \
  -v $(pwd)/models:/models \
  -p 5800:5800 \
  claracore:universal
```

## üìÅ Folder Structure

```
docker/
‚îú‚îÄ‚îÄ models/              # Put your GGUF models here
‚îú‚îÄ‚îÄ config/              # Auto-generated config (persisted)
‚îú‚îÄ‚îÄ binaries/            # Cached llama.cpp binaries
‚îú‚îÄ‚îÄ downloads/           # Model downloads cache
‚îú‚îÄ‚îÄ Dockerfile.ollama-style    # The universal Dockerfile
‚îú‚îÄ‚îÄ entrypoint-universal.sh    # Smart startup script
‚îú‚îÄ‚îÄ docker-compose.*.yml       # Various deployment configs
‚îî‚îÄ‚îÄ DEPLOYMENT.md              # Full documentation
```

## üîç Hardware Detection Example

When you start the container, you'll see:

```
üöÄ ClaraCore Universal Container Starting...
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
üîç Detecting available hardware...

‚úÖ NVIDIA GPU detected
NVIDIA GeForce RTX 4090, 24564 MiB

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
üéØ Selected Backend: cuda
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

üì¶ Models detected - auto-generating configuration...
üîß Starting with auto-setup mode...
Clara Core listening on :5800
```

## üõ†Ô∏è Advanced Usage

### Custom Port

```bash
# Run on port 8080 instead of 5800
docker run -p 8080:5800 \
  -v $(pwd)/models:/models \
  claracore:universal
```

### Multiple GPU Selection

```bash
# NVIDIA: Use specific GPUs
docker run -e NVIDIA_VISIBLE_DEVICES=0,1 \
  -v $(pwd)/models:/models \
  claracore:universal

# AMD: Use specific GPUs
docker run -e HIP_VISIBLE_DEVICES=0,1 \
  -v $(pwd)/models:/models \
  claracore:universal
```

### Override Hardware Resources

```bash
# Force specific VRAM/RAM amounts
docker run \
  -e CLARACORE_BACKEND=cuda \
  -v $(pwd)/models:/models \
  claracore:universal \
  --listen :5800 \
  --models-folder /models \
  --config /app/config/config.yaml \
  --vram 16 \
  --ram 32
```

## üêõ Troubleshooting

### Container starts but uses CPU instead of GPU

**Check GPU access:**

```bash
# For NVIDIA
docker run --rm --gpus all nvidia/cuda:12.0-base nvidia-smi

# For AMD
docker run --rm --device=/dev/kfd --device=/dev/dri rocm/rocm-terminal:latest rocm-smi

# For Vulkan
docker run --rm --device=/dev/dri ubuntu:22.04 apt-get update && apt-get install -y vulkan-tools && vulkaninfo
```

If these fail, you need to install GPU support:
- NVIDIA: [nvidia-container-toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html)
- AMD: [ROCm drivers](https://rocm.docs.amd.com/en/latest/deploy/linux/quick_start.html)

### Force CPU mode for testing

```bash
docker run -e CLARACORE_BACKEND=cpu \
  -v $(pwd)/models:/models \
  -p 5800:5800 \
  claracore:universal
```

### View detection logs

```bash
docker logs claracore 2>&1 | grep -A 20 "Detecting"
```

### No models found

Ensure your models folder has `.gguf` files:

```bash
ls -lh docker/models/*.gguf
```

## üìä Performance Expectations

### NVIDIA RTX 4090 (CUDA)
- Llama 3 8B: ~100 tok/s
- Llama 3 70B: ~20 tok/s

### AMD RX 7900 XTX (ROCm)
- Llama 3 8B: ~85 tok/s
- Llama 3 70B: ~15 tok/s

### Any GPU (Vulkan)
- Llama 3 8B: ~60-70 tok/s
- Llama 3 70B: ~10-12 tok/s

### CPU (AMD Ryzen 9 5950X)
- Llama 3 8B: ~15 tok/s
- Llama 3 70B: ~2-3 tok/s

*Performance varies based on model quantization and hardware*

## üéì How It Works

1. **Container starts** ‚Üí Runs `entrypoint-universal.sh`
2. **Hardware detection** ‚Üí Checks for NVIDIA, AMD, Vulkan
3. **Backend selection** ‚Üí Picks the fastest available
4. **Binary management** ‚Üí ClaraCore downloads correct llama.cpp binary
5. **Auto-configuration** ‚Üí Scans models and generates optimal config
6. **Server start** ‚Üí Listens on port 5800

The beauty is: **All of this happens automatically!**

## üìù Files Overview

| File | Purpose |
|------|---------|
| `Dockerfile.ollama-style` | Universal container definition |
| `entrypoint-universal.sh` | Smart hardware detection script |
| `docker-compose.universal.yml` | Auto-detect compose file |
| `docker-compose.cpu-only.yml` | Force CPU mode |
| `docker-compose.cuda-explicit.yml` | Force CUDA (NVIDIA) |
| `docker-compose.rocm-explicit.yml` | Force ROCm (AMD) |
| `docker-compose.vulkan-explicit.yml` | Force Vulkan |
| `build-universal.sh` | Build script (Linux/Mac) |
| `build-universal.ps1` | Build script (Windows) |
| `test-universal.sh` | Quick test script |
| `DEPLOYMENT.md` | Full deployment guide |

## üéØ Best Practices

1. **Use auto-detection** unless you have a specific reason not to
2. **Mount volumes** for models, config, and binaries (faster restarts)
3. **Check logs** on first run to see what hardware was detected
4. **Test with CPU first** if you're having GPU issues
5. **Keep models folder clean** - only GGUF files

## üöÄ Production Deployment

### Docker Compose (Recommended)

```bash
# Start in detached mode
docker compose -f docker-compose.universal.yml up -d

# Check status
docker compose -f docker-compose.universal.yml ps

# View logs
docker compose -f docker-compose.universal.yml logs -f

# Stop
docker compose -f docker-compose.universal.yml down
```

### Docker CLI

```bash
docker run -d \
  --name claracore \
  --restart unless-stopped \
  -p 5800:5800 \
  -v /path/to/models:/models \
  -v /path/to/config:/app/config \
  --gpus all \
  claracore:universal
```

### Kubernetes

See `DEPLOYMENT.md` for Kubernetes deployment examples.

## üí° Pro Tips

1. **First run is slower** - Downloads binaries, generates config
2. **Subsequent starts are fast** - Everything is cached
3. **One image for all environments** - Build once, deploy anywhere
4. **Hardware changes? No problem** - Container adapts automatically
5. **Override when needed** - But auto-detection usually just works

## üéâ That's It!

You now have a **truly universal** container that works on:
- ‚úÖ NVIDIA GPUs (CUDA)
- ‚úÖ AMD GPUs (ROCm)
- ‚úÖ Intel GPUs (Vulkan)
- ‚úÖ Any GPU with Vulkan
- ‚úÖ CPU-only systems

**No configuration, no hassle, it just works!**

For more details, see [DEPLOYMENT.md](DEPLOYMENT.md)
