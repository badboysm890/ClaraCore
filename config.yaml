groups:
    large-models:
        exclusive: true
        members:
            - qwen3-0.6b.q2-k
            - tinyllama-1.1b-chat-v1.0.q4-0
        startPort: 5800
        swap: true
healthCheckTimeout: 300
logLevel: info
macros:
    llama-embed-base: binaries\llama-server\llama-server.exe --host 127.0.0.1 --port ${PORT} --embedding
    llama-server-base: binaries\llama-server\llama-server.exe --host 127.0.0.1 --port ${PORT} --metrics --flash-attn auto --no-warmup --dry-penalty-last-n 0 --batch-size 2048 --ubatch-size 512
models:
    qwen3-0.6b.q2-k:
        cmd: |-
            ${llama-server-base}
                  --model C:\Users\prave\ClaraCore\downloads\Qwen3-0.6B.Q2_K.gguf
                  --ctx-size 8192
                  -ngl 999
                  --cache-type-k q4_0
                  --cache-type-v q4_0
                  --jinja
                  --batch-size 2048
                  --ubatch-size 1024
                  --temp 0.7
                  --repeat-penalty 1.05
                  --repeat-last-n 256
                  --top-p 0.9
                  --top-k 40
                  --min-p 0.1
        description: Auto-added from downloads - Qwen3-0.6B.Q2_K
        env:
            - CUDA_VISIBLE_DEVICES=0
        modelId: qwen3-0.6b.q2-k
        name: Qwen3-0.6B.Q2_K
        proxy: http://127.0.0.1:${PORT}
    tinyllama-1.1b-chat-v1.0.q4-0:
        cmd: |-
            ${llama-server-base}
                  --model C:\Users\prave\ClaraCore\downloads\tinyllama-1.1b-chat-v1.0.Q4_0.gguf
                  --ctx-size 8192
                  -ngl 999
                  --cache-type-k q4_0
                  --cache-type-v q4_0
                  --jinja
                  --batch-size 2048
                  --ubatch-size 1024
                  --temp 0.7
                  --repeat-penalty 1.05
                  --repeat-last-n 256
                  --top-p 0.9
                  --top-k 40
                  --min-p 0.1
        description: Auto-added from downloads - tinyllama-1.1b-chat-v1.0.Q4_0
        env:
            - CUDA_VISIBLE_DEVICES=0
        modelId: tinyllama-1.1b-chat-v1.0.q4-0
        name: tinyllama-1.1b-chat-v1.0.Q4_0
        proxy: http://127.0.0.1:${PORT}
startPort: 5800
